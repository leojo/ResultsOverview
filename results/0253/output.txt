Sender: LSF System <lsfadmin@lo-s4-036>
Subject: Job 583493: <python main.py results/0253> in cluster <leonhard> Exited

Job <python main.py results/0253> was submitted from host <lo-login-01> by user <leoj> in cluster <leonhard> at Sun Jul 29 21:47:21 2018
Job was executed on host(s) <4*lo-s4-036>, in queue <gpu.24h>, as user <leoj> in cluster <leonhard> at Mon Jul 30 00:00:44 2018
</cluster/home/leoj> was used as the home directory.
</cluster/home/leoj/sandbox> was used as the working directory.
Started at Mon Jul 30 00:00:44 2018
Terminated at Mon Jul 30 02:28:49 2018
Results reported at Mon Jul 30 02:28:49 2018

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py results/0253
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   10314.72 sec.
    Max Memory :                                 49316 MB
    Average Memory :                             8311.33 MB
    Total Requested Memory :                     128000.00 MB
    Delta Memory :                               78684.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                27
    Run time :                                   8892 sec.
    Turnaround time :                            16888 sec.

The output (if any) follows:

WARNING:tensorflow:From /cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From /cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.
Instructions for updating:
`NHWC` for data_format is deprecated, use `NWC` instead
2018-07-30 00:08:45.585128: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-30 00:08:45.937304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-07-30 00:08:45.937351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-07-30 00:08:47.029439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-30 00:08:47.029516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-07-30 00:08:47.029530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-07-30 00:08:47.030062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
  0%|          | 0/600 [00:00<?, ?it/s]Loss: 0.0129, L2-loss: 0.0129, KL: 0.0215, Learning rate: 0.001:   0%|          | 0/600 [00:37<?, ?it/s]Loss: 0.0129, L2-loss: 0.0129, KL: 0.0215, Learning rate: 0.001:   0%|          | 1/600 [00:37<6:16:05, 37.67s/it]Loss: 0.0129, L2-loss: 0.0129, KL: 0.5650, Learning rate: 0.001:   0%|          | 1/600 [01:12<12:05:07, 72.63s/it]Loss: 0.0129, L2-loss: 0.0129, KL: 0.5650, Learning rate: 0.001:   0%|          | 2/600 [01:12<6:01:57, 36.32s/it] Loss: 0.0115, L2-loss: 0.0112, KL: 2.7388, Learning rate: 0.001:   0%|          | 2/600 [01:47<8:54:34, 53.64s/it]Loss: 0.0115, L2-loss: 0.0112, KL: 2.7388, Learning rate: 0.001:   0%|          | 3/600 [01:47<5:55:47, 35.76s/it]Loss: 0.0104, L2-loss: 0.0100, KL: 3.7914, Learning rate: 0.001:   0%|          | 3/600 [02:21<7:48:57, 47.13s/it]Loss: 0.0104, L2-loss: 0.0100, KL: 3.7914, Learning rate: 0.001:   1%|          | 4/600 [02:21<5:51:07, 35.35s/it]Loss: 0.0095, L2-loss: 0.0091, KL: 4.5664, Learning rate: 0.001:   1%|          | 4/600 [02:55<7:15:40, 43.86s/it]Loss: 0.0095, L2-loss: 0.0091, KL: 4.5664, Learning rate: 0.001:   1%|          | 5/600 [02:55<5:47:57, 35.09s/it]Loss: 0.0089, L2-loss: 0.0084, KL: 5.0655, Learning rate: 0.001:   1%|          | 5/600 [03:29<6:56:16, 41.98s/it]Loss: 0.0089, L2-loss: 0.0084, KL: 5.0655, Learning rate: 0.001:   1%|1         | 6/600 [03:29<5:46:18, 34.98s/it]Loss: 0.0083, L2-loss: 0.0077, KL: 5.2508, Learning rate: 0.001:   1%|1         | 6/600 [04:04<6:42:43, 40.68s/it]Loss: 0.0083, L2-loss: 0.0077, KL: 5.2508, Learning rate: 0.001:   1%|1         | 7/600 [04:04<5:44:36, 34.87s/it]Loss: 0.0079, L2-loss: 0.0073, KL: 5.3742, Learning rate: 0.001:   1%|1         | 7/600 [04:37<6:32:26, 39.71s/it]Loss: 0.0079, L2-loss: 0.0073, KL: 5.3742, Learning rate: 0.001:   1%|1         | 8/600 [04:37<5:42:48, 34.74s/it]Loss: 0.0075, L2-loss: 0.0070, KL: 5.4298, Learning rate: 0.001:   1%|1         | 8/600 [05:12<6:25:06, 39.03s/it]Loss: 0.0075, L2-loss: 0.0070, KL: 5.4298, Learning rate: 0.001:   2%|1         | 9/600 [05:12<5:41:44, 34.69s/it]Loss: 0.0074, L2-loss: 0.0068, KL: 5.5392, Learning rate: 0.001:   2%|1         | 9/600 [05:46<6:19:26, 38.52s/it]Loss: 0.0074, L2-loss: 0.0068, KL: 5.5392, Learning rate: 0.001:   2%|1         | 10/600 [05:46<5:40:55, 34.67s/it]Loss: 0.0071, L2-loss: 0.0066, KL: 5.5111, Learning rate: 0.001:   2%|1         | 10/600 [06:20<6:14:32, 38.09s/it]Loss: 0.0071, L2-loss: 0.0066, KL: 5.5111, Learning rate: 0.001:   2%|1         | 11/600 [06:20<5:39:54, 34.63s/it]Loss: 0.0069, L2-loss: 0.0064, KL: 5.5148, Learning rate: 0.001:   2%|1         | 11/600 [06:55<6:10:24, 37.73s/it]Loss: 0.0069, L2-loss: 0.0064, KL: 5.5148, Learning rate: 0.001:   2%|2         | 12/600 [06:55<5:38:57, 34.59s/it]Loss: 0.0069, L2-loss: 0.0063, KL: 5.5862, Learning rate: 0.001:   2%|2         | 12/600 [07:29<6:07:20, 37.48s/it]Loss: 0.0069, L2-loss: 0.0063, KL: 5.5862, Learning rate: 0.001:   2%|2         | 13/600 [07:29<5:38:30, 34.60s/it]Loss: 0.0068, L2-loss: 0.0062, KL: 5.5849, Learning rate: 0.001:   2%|2         | 13/600 [08:04<6:04:31, 37.26s/it]Loss: 0.0068, L2-loss: 0.0062, KL: 5.5849, Learning rate: 0.001:   2%|2         | 14/600 [08:04<5:37:54, 34.60s/it]Loss: 0.0067, L2-loss: 0.0061, KL: 5.6139, Learning rate: 0.001:   2%|2         | 14/600 [08:38<6:02:03, 37.07s/it]Loss: 0.0067, L2-loss: 0.0061, KL: 5.6139, Learning rate: 0.001:   2%|2         | 15/600 [08:38<5:37:20, 34.60s/it]Loss: 0.0065, L2-loss: 0.0059, KL: 5.5570, Learning rate: 0.001:   2%|2         | 15/600 [09:14<6:00:09, 36.94s/it]Loss: 0.0065, L2-loss: 0.0059, KL: 5.5570, Learning rate: 0.001:   3%|2         | 16/600 [09:14<5:37:04, 34.63s/it]Loss: 0.0064, L2-loss: 0.0059, KL: 5.5613, Learning rate: 0.001:   3%|2         | 16/600 [09:48<5:57:52, 36.77s/it]Loss: 0.0064, L2-loss: 0.0059, KL: 5.5613, Learning rate: 0.001:   3%|2         | 17/600 [09:48<5:36:14, 34.61s/it]Loss: 0.0061, L2-loss: 0.0056, KL: 5.4456, Learning rate: 0.001:   3%|2         | 17/600 [10:22<5:56:03, 36.64s/it]Loss: 0.0061, L2-loss: 0.0056, KL: 5.4456, Learning rate: 0.001:   3%|3         | 18/600 [10:22<5:35:42, 34.61s/it]Loss: 0.0061, L2-loss: 0.0056, KL: 5.4713, Learning rate: 0.001:   3%|3         | 18/600 [10:57<5:54:25, 36.54s/it]Loss: 0.0061, L2-loss: 0.0056, KL: 5.4713, Learning rate: 0.001:   3%|3         | 19/600 [10:57<5:35:11, 34.62s/it]Loss: 0.0061, L2-loss: 0.0055, KL: 5.4950, Learning rate: 0.001:   3%|3         | 19/600 [11:32<5:52:52, 36.44s/it]Loss: 0.0061, L2-loss: 0.0055, KL: 5.4950, Learning rate: 0.001:   3%|3         | 20/600 [11:32<5:34:38, 34.62s/it]Loss: 0.0058, L2-loss: 0.0053, KL: 5.3592, Learning rate: 0.001:   3%|3         | 20/600 [12:06<5:51:11, 36.33s/it]Loss: 0.0058, L2-loss: 0.0053, KL: 5.3592, Learning rate: 0.001:   4%|3         | 21/600 [12:06<5:33:53, 34.60s/it]Loss: 0.0059, L2-loss: 0.0053, KL: 5.4780, Learning rate: 0.001:   4%|3         | 21/600 [12:41<5:49:48, 36.25s/it]Loss: 0.0059, L2-loss: 0.0053, KL: 5.4780, Learning rate: 0.001:   4%|3         | 22/600 [12:41<5:33:19, 34.60s/it]Loss: 0.0058, L2-loss: 0.0053, KL: 5.4471, Learning rate: 0.001:   4%|3         | 22/600 [13:15<5:48:29, 36.17s/it]Loss: 0.0058, L2-loss: 0.0053, KL: 5.4471, Learning rate: 0.001:   4%|3         | 23/600 [13:15<5:32:45, 34.60s/it]Loss: 0.0057, L2-loss: 0.0052, KL: 5.3770, Learning rate: 0.001:   4%|3         | 23/600 [13:50<5:47:03, 36.09s/it]Loss: 0.0057, L2-loss: 0.0052, KL: 5.3770, Learning rate: 0.001:   4%|4         | 24/600 [13:50<5:32:01, 34.59s/it]Loss: 0.0057, L2-loss: 0.0051, KL: 5.3621, Learning rate: 0.001:   4%|4         | 24/600 [14:24<5:45:53, 36.03s/it]Loss: 0.0057, L2-loss: 0.0051, KL: 5.3621, Learning rate: 0.001:   4%|4         | 25/600 [14:24<5:31:29, 34.59s/it]Loss: 0.0056, L2-loss: 0.0050, KL: 5.3583, Learning rate: 0.001:   4%|4         | 25/600 [14:59<5:44:44, 35.97s/it]Loss: 0.0056, L2-loss: 0.0050, KL: 5.3583, Learning rate: 0.001:   4%|4         | 26/600 [14:59<5:30:54, 34.59s/it]Loss: 0.0055, L2-loss: 0.0050, KL: 5.3607, Learning rate: 0.001:   4%|4         | 26/600 [15:34<5:43:50, 35.94s/it]Loss: 0.0055, L2-loss: 0.0050, KL: 5.3607, Learning rate: 0.001:   4%|4         | 27/600 [15:34<5:30:31, 34.61s/it]Loss: 0.0055, L2-loss: 0.0049, KL: 5.3554, Learning rate: 0.001:   4%|4         | 27/600 [16:08<5:42:41, 35.88s/it]Loss: 0.0055, L2-loss: 0.0049, KL: 5.3554, Learning rate: 0.001:   5%|4         | 28/600 [16:08<5:29:52, 34.60s/it]Loss: 0.0053, L2-loss: 0.0048, KL: 5.2767, Learning rate: 0.001:   5%|4         | 28/600 [16:43<5:41:38, 35.84s/it]Loss: 0.0053, L2-loss: 0.0048, KL: 5.2767, Learning rate: 0.001:   5%|4         | 29/600 [16:43<5:29:17, 34.60s/it]Loss: 0.0053, L2-loss: 0.0047, KL: 5.2453, Learning rate: 0.001:   5%|4         | 29/600 [17:18<5:40:39, 35.80s/it]Loss: 0.0053, L2-loss: 0.0047, KL: 5.2453, Learning rate: 0.001:   5%|5         | 30/600 [17:18<5:28:43, 34.60s/it]Loss: 0.0052, L2-loss: 0.0047, KL: 5.2660, Learning rate: 0.001:   5%|5         | 30/600 [17:53<5:39:58, 35.79s/it]Loss: 0.0052, L2-loss: 0.0047, KL: 5.2660, Learning rate: 0.001:   5%|5         | 31/600 [17:53<5:28:26, 34.63s/it]Loss: 0.0051, L2-loss: 0.0046, KL: 5.2172, Learning rate: 0.001:   5%|5         | 31/600 [18:28<5:39:07, 35.76s/it]Loss: 0.0051, L2-loss: 0.0046, KL: 5.2172, Learning rate: 0.001:   5%|5         | 32/600 [18:28<5:27:57, 34.64s/it]Loss: 0.0050, L2-loss: 0.0045, KL: 5.2328, Learning rate: 0.001:   5%|5         | 32/600 [19:03<5:38:24, 35.75s/it]Loss: 0.0050, L2-loss: 0.0045, KL: 5.2328, Learning rate: 0.001:   6%|5         | 33/600 [19:03<5:27:34, 34.66s/it]Loss: 0.0049, L2-loss: 0.0044, KL: 5.1602, Learning rate: 0.001:   6%|5         | 33/600 [19:39<5:37:41, 35.73s/it]Loss: 0.0049, L2-loss: 0.0044, KL: 5.1602, Learning rate: 0.001:   6%|5         | 34/600 [19:39<5:27:11, 34.68s/it]Loss: 0.0050, L2-loss: 0.0045, KL: 5.2096, Learning rate: 0.001:   6%|5         | 34/600 [20:13<5:36:44, 35.70s/it]Loss: 0.0050, L2-loss: 0.0045, KL: 5.2096, Learning rate: 0.001:   6%|5         | 35/600 [20:13<5:26:32, 34.68s/it]Loss: 0.0049, L2-loss: 0.0044, KL: 5.2028, Learning rate: 0.001:   6%|5         | 35/600 [20:48<5:36:01, 35.68s/it]Loss: 0.0049, L2-loss: 0.0044, KL: 5.2028, Learning rate: 0.001:   6%|6         | 36/600 [20:48<5:26:07, 34.69s/it]Loss: 0.0048, L2-loss: 0.0043, KL: 5.2035, Learning rate: 0.001:   6%|6         | 36/600 [21:23<5:35:12, 35.66s/it]Loss: 0.0048, L2-loss: 0.0043, KL: 5.2035, Learning rate: 0.001:   6%|6         | 37/600 [21:23<5:25:34, 34.70s/it]Loss: 0.0047, L2-loss: 0.0042, KL: 5.1497, Learning rate: 0.001:   6%|6         | 37/600 [21:58<5:34:23, 35.64s/it]Loss: 0.0047, L2-loss: 0.0042, KL: 5.1497, Learning rate: 0.001:   6%|6         | 38/600 [21:58<5:25:01, 34.70s/it]Loss: 0.0047, L2-loss: 0.0042, KL: 5.1499, Learning rate: 0.001:   6%|6         | 38/600 [22:33<5:33:32, 35.61s/it]Loss: 0.0047, L2-loss: 0.0042, KL: 5.1499, Learning rate: 0.001:   6%|6         | 39/600 [22:33<5:24:25, 34.70s/it]Loss: 0.0047, L2-loss: 0.0042, KL: 5.1494, Learning rate: 0.001:   6%|6         | 39/600 [23:07<5:32:39, 35.58s/it]Loss: 0.0047, L2-loss: 0.0042, KL: 5.1494, Learning rate: 0.001:   7%|6         | 40/600 [23:07<5:23:45, 34.69s/it]Loss: 0.0046, L2-loss: 0.0041, KL: 5.1732, Learning rate: 0.001:   7%|6         | 40/600 [23:41<5:31:44, 35.54s/it]Loss: 0.0046, L2-loss: 0.0041, KL: 5.1732, Learning rate: 0.001:   7%|6         | 41/600 [23:41<5:23:04, 34.68s/it]Loss: 0.0045, L2-loss: 0.0040, KL: 5.0905, Learning rate: 0.001:   7%|6         | 41/600 [24:16<5:30:54, 35.52s/it]Loss: 0.0045, L2-loss: 0.0040, KL: 5.0905, Learning rate: 0.001:   7%|7         | 42/600 [24:16<5:22:27, 34.67s/it]Loss: 0.0044, L2-loss: 0.0039, KL: 5.0638, Learning rate: 0.001:   7%|7         | 42/600 [24:50<5:30:05, 35.49s/it]Loss: 0.0044, L2-loss: 0.0039, KL: 5.0638, Learning rate: 0.001:   7%|7         | 43/600 [24:50<5:21:49, 34.67s/it]Loss: 0.0044, L2-loss: 0.0039, KL: 5.0534, Learning rate: 0.001:   7%|7         | 43/600 [25:24<5:29:10, 35.46s/it]Loss: 0.0044, L2-loss: 0.0039, KL: 5.0534, Learning rate: 0.001:   7%|7         | 44/600 [25:24<5:21:07, 34.65s/it]Loss: 0.0044, L2-loss: 0.0039, KL: 5.0274, Learning rate: 0.001:   7%|7         | 44/600 [25:58<5:28:16, 35.43s/it]Loss: 0.0044, L2-loss: 0.0039, KL: 5.0274, Learning rate: 0.001:   8%|7         | 45/600 [25:58<5:20:24, 34.64s/it]Loss: 0.0043, L2-loss: 0.0038, KL: 5.0229, Learning rate: 0.001:   8%|7         | 45/600 [26:32<5:27:26, 35.40s/it]Loss: 0.0043, L2-loss: 0.0038, KL: 5.0229, Learning rate: 0.001:   8%|7         | 46/600 [26:32<5:19:45, 34.63s/it]Loss: 0.0043, L2-loss: 0.0038, KL: 5.0208, Learning rate: 0.001:   8%|7         | 46/600 [27:07<5:26:39, 35.38s/it]Loss: 0.0043, L2-loss: 0.0038, KL: 5.0208, Learning rate: 0.001:   8%|7         | 47/600 [27:07<5:19:08, 34.63s/it]Loss: 0.0043, L2-loss: 0.0038, KL: 5.0450, Learning rate: 0.001:   8%|7         | 47/600 [27:41<5:25:53, 35.36s/it]Loss: 0.0043, L2-loss: 0.0038, KL: 5.0450, Learning rate: 0.001:   8%|8         | 48/600 [27:41<5:18:31, 34.62s/it]Loss: 0.0042, L2-loss: 0.0037, KL: 5.0105, Learning rate: 0.001:   8%|8         | 48/600 [28:15<5:25:03, 35.33s/it]Loss: 0.0042, L2-loss: 0.0037, KL: 5.0105, Learning rate: 0.001:   8%|8         | 49/600 [28:15<5:17:51, 34.61s/it]Loss: 0.0042, L2-loss: 0.0037, KL: 5.0090, Learning rate: 0.001:   8%|8         | 49/600 [28:50<5:24:18, 35.31s/it]Loss: 0.0042, L2-loss: 0.0037, KL: 5.0090, Learning rate: 0.001:   8%|8         | 50/600 [28:50<5:17:14, 34.61s/it]Loss: 0.0042, L2-loss: 0.0037, KL: 4.9949, Learning rate: 0.001:   8%|8         | 50/600 [29:23<5:23:20, 35.27s/it]Loss: 0.0042, L2-loss: 0.0037, KL: 4.9949, Learning rate: 0.001:   8%|8         | 51/600 [29:23<5:16:25, 34.58s/it]Loss: 0.0041, L2-loss: 0.0036, KL: 5.0059, Learning rate: 0.001:   8%|8         | 51/600 [29:57<5:22:34, 35.25s/it]Loss: 0.0041, L2-loss: 0.0036, KL: 5.0059, Learning rate: 0.001:   9%|8         | 52/600 [29:57<5:15:47, 34.58s/it]Loss: 0.0041, L2-loss: 0.0036, KL: 4.9835, Learning rate: 0.001:   9%|8         | 52/600 [30:32<5:21:50, 35.24s/it]Loss: 0.0041, L2-loss: 0.0036, KL: 4.9835, Learning rate: 0.001:   9%|8         | 53/600 [30:32<5:15:11, 34.57s/it]Loss: 0.0040, L2-loss: 0.0035, KL: 4.9573, Learning rate: 0.001:   9%|8         | 53/600 [31:07<5:21:11, 35.23s/it]Loss: 0.0040, L2-loss: 0.0035, KL: 4.9573, Learning rate: 0.001:   9%|9         | 54/600 [31:07<5:14:40, 34.58s/it]Loss: 0.0040, L2-loss: 0.0035, KL: 4.9260, Learning rate: 0.001:   9%|9         | 54/600 [31:41<5:20:27, 35.22s/it]Loss: 0.0040, L2-loss: 0.0035, KL: 4.9260, Learning rate: 0.001:   9%|9         | 55/600 [31:41<5:14:03, 34.58s/it]Loss: 0.0040, L2-loss: 0.0035, KL: 4.9271, Learning rate: 0.001:   9%|9         | 55/600 [32:16<5:19:44, 35.20s/it]Loss: 0.0040, L2-loss: 0.0035, KL: 4.9271, Learning rate: 0.001:   9%|9         | 56/600 [32:16<5:13:27, 34.57s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8749, Learning rate: 0.001:   9%|9         | 56/600 [32:50<5:19:01, 35.19s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8749, Learning rate: 0.001:  10%|9         | 57/600 [32:50<5:12:50, 34.57s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8382, Learning rate: 0.001:  10%|9         | 57/600 [33:24<5:18:18, 35.17s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8382, Learning rate: 0.001:  10%|9         | 58/600 [33:24<5:12:14, 34.57s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8341, Learning rate: 0.001:  10%|9         | 58/600 [33:59<5:17:36, 35.16s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8341, Learning rate: 0.001:  10%|9         | 59/600 [33:59<5:11:39, 34.56s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8584, Learning rate: 0.001:  10%|9         | 59/600 [34:33<5:16:54, 35.15s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8584, Learning rate: 0.001:  10%|#         | 60/600 [34:33<5:11:03, 34.56s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8342, Learning rate: 0.001:  10%|#         | 60/600 [35:07<5:16:06, 35.12s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8342, Learning rate: 0.001:  10%|#         | 61/600 [35:07<5:10:21, 34.55s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8257, Learning rate: 0.001:  10%|#         | 61/600 [35:41<5:15:24, 35.11s/it]Loss: 0.0038, L2-loss: 0.0033, KL: 4.8257, Learning rate: 0.001:  10%|#         | 62/600 [35:41<5:09:45, 34.54s/it]Loss: 0.0037, L2-loss: 0.0033, KL: 4.8242, Learning rate: 0.001:  10%|#         | 62/600 [36:16<5:14:45, 35.10s/it]Loss: 0.0037, L2-loss: 0.0033, KL: 4.8242, Learning rate: 0.001:  10%|#         | 63/600 [36:16<5:09:11, 34.55s/it]Loss: 0.0037, L2-loss: 0.0032, KL: 4.7919, Learning rate: 0.001:  10%|#         | 63/600 [36:51<5:14:08, 35.10s/it]Loss: 0.0037, L2-loss: 0.0032, KL: 4.7919, Learning rate: 0.001:  11%|#         | 64/600 [36:51<5:08:39, 34.55s/it]Loss: 0.0037, L2-loss: 0.0032, KL: 4.7717, Learning rate: 0.001:  11%|#         | 64/600 [37:26<5:13:31, 35.10s/it]Loss: 0.0037, L2-loss: 0.0032, KL: 4.7717, Learning rate: 0.001:  11%|#         | 65/600 [37:26<5:08:07, 34.56s/it]Loss: 0.0037, L2-loss: 0.0032, KL: 4.7691, Learning rate: 0.001:  11%|#         | 65/600 [38:01<5:12:57, 35.10s/it]Loss: 0.0037, L2-loss: 0.0032, KL: 4.7691, Learning rate: 0.001:  11%|#1        | 66/600 [38:01<5:07:38, 34.57s/it]Loss: 0.0036, L2-loss: 0.0032, KL: 4.7885, Learning rate: 0.001:  11%|#1        | 66/600 [38:36<5:12:21, 35.10s/it]Loss: 0.0036, L2-loss: 0.0032, KL: 4.7885, Learning rate: 0.001:  11%|#1        | 67/600 [38:36<5:07:06, 34.57s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7301, Learning rate: 0.001:  11%|#1        | 67/600 [39:11<5:11:46, 35.10s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7301, Learning rate: 0.001:  11%|#1        | 68/600 [39:11<5:06:36, 34.58s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7210, Learning rate: 0.001:  11%|#1        | 68/600 [39:46<5:11:09, 35.09s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7210, Learning rate: 0.001:  12%|#1        | 69/600 [39:46<5:06:04, 34.58s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7116, Learning rate: 0.001:  12%|#1        | 69/600 [40:21<5:10:34, 35.09s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7116, Learning rate: 0.001:  12%|#1        | 70/600 [40:21<5:05:33, 34.59s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7331, Learning rate: 0.001:  12%|#1        | 70/600 [40:55<5:09:55, 35.09s/it]Loss: 0.0036, L2-loss: 0.0031, KL: 4.7331, Learning rate: 0.001:  12%|#1        | 71/600 [40:55<5:04:58, 34.59s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6630, Learning rate: 0.001:  12%|#1        | 71/600 [41:30<5:09:18, 35.08s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6630, Learning rate: 0.001:  12%|#2        | 72/600 [41:30<5:04:25, 34.59s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6738, Learning rate: 0.001:  12%|#2        | 72/600 [42:06<5:08:44, 35.08s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6738, Learning rate: 0.001:  12%|#2        | 73/600 [42:06<5:03:56, 34.60s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6628, Learning rate: 0.001:  12%|#2        | 73/600 [42:40<5:08:06, 35.08s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6628, Learning rate: 0.001:  12%|#2        | 74/600 [42:40<5:03:22, 34.61s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6621, Learning rate: 0.001:  12%|#2        | 74/600 [43:16<5:07:32, 35.08s/it]Loss: 0.0035, L2-loss: 0.0030, KL: 4.6621, Learning rate: 0.001:  12%|#2        | 75/600 [43:16<5:02:52, 34.61s/it]Loss: 0.0034, L2-loss: 0.0030, KL: 4.6568, Learning rate: 0.001:  12%|#2        | 75/600 [43:50<5:06:56, 35.08s/it]Loss: 0.0034, L2-loss: 0.0030, KL: 4.6568, Learning rate: 0.001:  13%|#2        | 76/600 [43:50<5:02:19, 34.62s/it]Loss: 0.0034, L2-loss: 0.0030, KL: 4.6231, Learning rate: 0.001:  13%|#2        | 76/600 [44:25<5:06:20, 35.08s/it]Loss: 0.0034, L2-loss: 0.0030, KL: 4.6231, Learning rate: 0.001:  13%|#2        | 77/600 [44:25<5:01:46, 34.62s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.6342, Learning rate: 0.001:  13%|#2        | 77/600 [45:00<5:05:40, 35.07s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.6342, Learning rate: 0.001:  13%|#3        | 78/600 [45:00<5:01:10, 34.62s/it]Loss: 0.0034, L2-loss: 0.0030, KL: 4.6133, Learning rate: 0.001:  13%|#3        | 78/600 [45:35<5:05:04, 35.07s/it]Loss: 0.0034, L2-loss: 0.0030, KL: 4.6133, Learning rate: 0.001:  13%|#3        | 79/600 [45:35<5:00:38, 34.62s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.5931, Learning rate: 0.001:  13%|#3        | 79/600 [46:10<5:04:28, 35.06s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.5931, Learning rate: 0.001:  13%|#3        | 80/600 [46:10<5:00:05, 34.63s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.6013, Learning rate: 0.001:  13%|#3        | 80/600 [46:44<5:03:50, 35.06s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.6013, Learning rate: 0.001:  14%|#3        | 81/600 [46:44<4:59:30, 34.63s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.5882, Learning rate: 0.001:  14%|#3        | 81/600 [47:19<5:03:11, 35.05s/it]Loss: 0.0034, L2-loss: 0.0029, KL: 4.5882, Learning rate: 0.001:  14%|#3        | 82/600 [47:19<4:58:55, 34.62s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5594, Learning rate: 0.001:  14%|#3        | 82/600 [47:53<5:02:31, 35.04s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5594, Learning rate: 0.001:  14%|#3        | 83/600 [47:53<4:58:17, 34.62s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5467, Learning rate: 0.001:  14%|#3        | 83/600 [48:27<5:01:48, 35.03s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5467, Learning rate: 0.001:  14%|#4        | 84/600 [48:27<4:57:38, 34.61s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5381, Learning rate: 0.001:  14%|#4        | 84/600 [49:02<5:01:12, 35.02s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5381, Learning rate: 0.001:  14%|#4        | 85/600 [49:02<4:57:05, 34.61s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5378, Learning rate: 0.001:  14%|#4        | 85/600 [49:36<5:00:34, 35.02s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5378, Learning rate: 0.001:  14%|#4        | 86/600 [49:36<4:56:29, 34.61s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5331, Learning rate: 0.001:  14%|#4        | 86/600 [50:11<4:59:56, 35.01s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5331, Learning rate: 0.001:  14%|#4        | 87/600 [50:11<4:55:54, 34.61s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5213, Learning rate: 0.001:  14%|#4        | 87/600 [50:44<4:59:14, 35.00s/it]Loss: 0.0033, L2-loss: 0.0028, KL: 4.5213, Learning rate: 0.001:  15%|#4        | 88/600 [50:44<4:55:16, 34.60s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5222, Learning rate: 0.001:  15%|#4        | 88/600 [51:19<4:58:35, 34.99s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5222, Learning rate: 0.001:  15%|#4        | 89/600 [51:19<4:54:39, 34.60s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5155, Learning rate: 0.001:  15%|#4        | 89/600 [51:53<4:57:56, 34.98s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5155, Learning rate: 0.001:  15%|#5        | 90/600 [51:53<4:54:03, 34.59s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4996, Learning rate: 0.001:  15%|#5        | 90/600 [52:28<4:57:19, 34.98s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4996, Learning rate: 0.001:  15%|#5        | 91/600 [52:28<4:53:29, 34.60s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5160, Learning rate: 0.001:  15%|#5        | 91/600 [53:02<4:56:41, 34.97s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5160, Learning rate: 0.001:  15%|#5        | 92/600 [53:02<4:52:53, 34.59s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5242, Learning rate: 0.001:  15%|#5        | 92/600 [53:37<4:56:05, 34.97s/it]Loss: 0.0032, L2-loss: 0.0028, KL: 4.5242, Learning rate: 0.001:  16%|#5        | 93/600 [53:37<4:52:19, 34.60s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4918, Learning rate: 0.001:  16%|#5        | 93/600 [54:12<4:55:32, 34.98s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4918, Learning rate: 0.001:  16%|#5        | 94/600 [54:12<4:51:49, 34.60s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4823, Learning rate: 0.001:  16%|#5        | 94/600 [54:47<4:54:55, 34.97s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4823, Learning rate: 0.001:  16%|#5        | 95/600 [54:47<4:51:14, 34.60s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4818, Learning rate: 0.001:  16%|#5        | 95/600 [55:22<4:54:21, 34.97s/it]Loss: 0.0032, L2-loss: 0.0027, KL: 4.4818, Learning rate: 0.001:  16%|#6        | 96/600 [55:22<4:50:42, 34.61s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4589, Learning rate: 0.001:  16%|#6        | 96/600 [55:56<4:53:42, 34.97s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4589, Learning rate: 0.001:  16%|#6        | 97/600 [55:56<4:50:06, 34.61s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4354, Learning rate: 0.001:  16%|#6        | 97/600 [56:31<4:53:09, 34.97s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4354, Learning rate: 0.001:  16%|#6        | 98/600 [56:31<4:49:35, 34.61s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4749, Learning rate: 0.001:  16%|#6        | 98/600 [57:06<4:52:34, 34.97s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4749, Learning rate: 0.001:  16%|#6        | 99/600 [57:06<4:49:02, 34.62s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4359, Learning rate: 0.001:  16%|#6        | 99/600 [57:41<4:51:59, 34.97s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4359, Learning rate: 0.001:  17%|#6        | 100/600 [57:41<4:48:29, 34.62s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3937, Learning rate: 0.001:  17%|#6        | 100/600 [58:16<4:51:23, 34.97s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3937, Learning rate: 0.001:  17%|#6        | 101/600 [58:16<4:47:56, 34.62s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.3979, Learning rate: 0.001:  17%|#6        | 101/600 [58:51<4:50:48, 34.97s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.3979, Learning rate: 0.001:  17%|#7        | 102/600 [58:51<4:47:22, 34.62s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4349, Learning rate: 0.001:  17%|#7        | 102/600 [59:26<4:50:11, 34.96s/it]Loss: 0.0031, L2-loss: 0.0027, KL: 4.4349, Learning rate: 0.001:  17%|#7        | 103/600 [59:26<4:46:48, 34.62s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.4104, Learning rate: 0.001:  17%|#7        | 103/600 [1:00:00<4:49:34, 34.96s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.4104, Learning rate: 0.001:  17%|#7        | 104/600 [1:00:00<4:46:13, 34.62s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.3934, Learning rate: 0.001:  17%|#7        | 104/600 [1:00:35<4:48:58, 34.96s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.3934, Learning rate: 0.001:  18%|#7        | 105/600 [1:00:35<4:45:38, 34.62s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.4108, Learning rate: 0.001:  18%|#7        | 105/600 [1:01:09<4:48:21, 34.95s/it]Loss: 0.0031, L2-loss: 0.0026, KL: 4.4108, Learning rate: 0.001:  18%|#7        | 106/600 [1:01:09<4:45:03, 34.62s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3544, Learning rate: 0.001:  18%|#7        | 106/600 [1:01:44<4:47:44, 34.95s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3544, Learning rate: 0.001:  18%|#7        | 107/600 [1:01:44<4:44:28, 34.62s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3762, Learning rate: 0.001:  18%|#7        | 107/600 [1:02:18<4:47:05, 34.94s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3762, Learning rate: 0.001:  18%|#8        | 108/600 [1:02:18<4:43:51, 34.62s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3594, Learning rate: 0.001:  18%|#8        | 108/600 [1:02:53<4:46:30, 34.94s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3594, Learning rate: 0.001:  18%|#8        | 109/600 [1:02:53<4:43:17, 34.62s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3711, Learning rate: 0.001:  18%|#8        | 109/600 [1:03:28<4:45:53, 34.94s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3711, Learning rate: 0.001:  18%|#8        | 110/600 [1:03:28<4:42:43, 34.62s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3704, Learning rate: 0.001:  18%|#8        | 110/600 [1:04:02<4:45:17, 34.93s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3704, Learning rate: 0.001:  18%|#8        | 111/600 [1:04:02<4:42:08, 34.62s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3246, Learning rate: 0.001:  18%|#8        | 111/600 [1:04:36<4:44:39, 34.93s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3246, Learning rate: 0.001:  19%|#8        | 112/600 [1:04:36<4:41:32, 34.61s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3224, Learning rate: 0.001:  19%|#8        | 112/600 [1:05:11<4:44:03, 34.92s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3224, Learning rate: 0.001:  19%|#8        | 113/600 [1:05:11<4:40:57, 34.62s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3177, Learning rate: 0.001:  19%|#8        | 113/600 [1:05:45<4:43:24, 34.92s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3177, Learning rate: 0.001:  19%|#9        | 114/600 [1:05:45<4:40:21, 34.61s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2880, Learning rate: 0.001:  19%|#9        | 114/600 [1:06:20<4:42:48, 34.92s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2880, Learning rate: 0.001:  19%|#9        | 115/600 [1:06:20<4:39:46, 34.61s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3338, Learning rate: 0.001:  19%|#9        | 115/600 [1:06:55<4:42:12, 34.91s/it]Loss: 0.0030, L2-loss: 0.0026, KL: 4.3338, Learning rate: 0.001:  19%|#9        | 116/600 [1:06:55<4:39:12, 34.61s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3222, Learning rate: 0.001:  19%|#9        | 116/600 [1:07:29<4:41:38, 34.91s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3222, Learning rate: 0.001:  20%|#9        | 117/600 [1:07:29<4:38:39, 34.61s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2869, Learning rate: 0.001:  20%|#9        | 117/600 [1:08:04<4:41:02, 34.91s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2869, Learning rate: 0.001:  20%|#9        | 118/600 [1:08:04<4:38:05, 34.62s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2581, Learning rate: 0.001:  20%|#9        | 118/600 [1:08:39<4:40:26, 34.91s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2581, Learning rate: 0.001:  20%|#9        | 119/600 [1:08:39<4:37:30, 34.62s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2867, Learning rate: 0.001:  20%|#9        | 119/600 [1:09:14<4:39:52, 34.91s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2867, Learning rate: 0.001:  20%|##        | 120/600 [1:09:14<4:36:57, 34.62s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3147, Learning rate: 0.001:  20%|##        | 120/600 [1:09:49<4:39:17, 34.91s/it]Loss: 0.0030, L2-loss: 0.0025, KL: 4.3147, Learning rate: 0.001:  20%|##        | 121/600 [1:09:49<4:36:24, 34.62s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2893, Learning rate: 0.001:  20%|##        | 121/600 [1:10:24<4:38:43, 34.91s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2893, Learning rate: 0.001:  20%|##        | 122/600 [1:10:24<4:35:52, 34.63s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2824, Learning rate: 0.001:  20%|##        | 122/600 [1:10:59<4:38:08, 34.91s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2824, Learning rate: 0.001:  20%|##        | 123/600 [1:10:59<4:35:17, 34.63s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2901, Learning rate: 0.001:  20%|##        | 123/600 [1:11:34<4:37:33, 34.91s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2901, Learning rate: 0.001:  21%|##        | 124/600 [1:11:34<4:34:44, 34.63s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2654, Learning rate: 0.001:  21%|##        | 124/600 [1:12:09<4:36:58, 34.91s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2654, Learning rate: 0.001:  21%|##        | 125/600 [1:12:09<4:34:11, 34.63s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2708, Learning rate: 0.001:  21%|##        | 125/600 [1:12:44<4:36:24, 34.92s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2708, Learning rate: 0.001:  21%|##1       | 126/600 [1:12:44<4:33:38, 34.64s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2330, Learning rate: 0.001:  21%|##1       | 126/600 [1:13:19<4:35:51, 34.92s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2330, Learning rate: 0.001:  21%|##1       | 127/600 [1:13:19<4:33:06, 34.64s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2384, Learning rate: 0.001:  21%|##1       | 127/600 [1:13:54<4:35:17, 34.92s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2384, Learning rate: 0.001:  21%|##1       | 128/600 [1:13:54<4:32:33, 34.65s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2436, Learning rate: 0.001:  21%|##1       | 128/600 [1:14:29<4:34:41, 34.92s/it]Loss: 0.0029, L2-loss: 0.0025, KL: 4.2436, Learning rate: 0.001:  22%|##1       | 129/600 [1:14:29<4:31:59, 34.65s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2239, Learning rate: 0.001:  22%|##1       | 129/600 [1:15:04<4:34:04, 34.91s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2239, Learning rate: 0.001:  22%|##1       | 130/600 [1:15:04<4:31:23, 34.65s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2326, Learning rate: 0.001:  22%|##1       | 130/600 [1:15:38<4:33:29, 34.91s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2326, Learning rate: 0.001:  22%|##1       | 131/600 [1:15:38<4:30:50, 34.65s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2459, Learning rate: 0.001:  22%|##1       | 131/600 [1:16:13<4:32:52, 34.91s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2459, Learning rate: 0.001:  22%|##2       | 132/600 [1:16:13<4:30:13, 34.65s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2190, Learning rate: 0.001:  22%|##2       | 132/600 [1:16:47<4:32:17, 34.91s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2190, Learning rate: 0.001:  22%|##2       | 133/600 [1:16:47<4:29:39, 34.65s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.2145, Learning rate: 0.001:  22%|##2       | 133/600 [1:17:22<4:31:40, 34.90s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.2145, Learning rate: 0.001:  22%|##2       | 134/600 [1:17:22<4:29:04, 34.64s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2147, Learning rate: 0.001:  22%|##2       | 134/600 [1:17:56<4:31:03, 34.90s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2147, Learning rate: 0.001:  22%|##2       | 135/600 [1:17:56<4:28:27, 34.64s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.2056, Learning rate: 0.001:  22%|##2       | 135/600 [1:18:31<4:30:27, 34.90s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.2056, Learning rate: 0.001:  23%|##2       | 136/600 [1:18:31<4:27:53, 34.64s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1815, Learning rate: 0.001:  23%|##2       | 136/600 [1:19:05<4:29:49, 34.89s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1815, Learning rate: 0.001:  23%|##2       | 137/600 [1:19:05<4:27:16, 34.64s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2200, Learning rate: 0.001:  23%|##2       | 137/600 [1:19:39<4:29:12, 34.89s/it]Loss: 0.0029, L2-loss: 0.0024, KL: 4.2200, Learning rate: 0.001:  23%|##3       | 138/600 [1:19:39<4:26:40, 34.63s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.2006, Learning rate: 0.001:  23%|##3       | 138/600 [1:20:13<4:28:36, 34.88s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.2006, Learning rate: 0.001:  23%|##3       | 139/600 [1:20:13<4:26:05, 34.63s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1940, Learning rate: 0.001:  23%|##3       | 139/600 [1:20:48<4:27:59, 34.88s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1940, Learning rate: 0.001:  23%|##3       | 140/600 [1:20:48<4:25:30, 34.63s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1555, Learning rate: 0.001:  23%|##3       | 140/600 [1:21:22<4:27:23, 34.88s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1555, Learning rate: 0.001:  24%|##3       | 141/600 [1:21:22<4:24:55, 34.63s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1495, Learning rate: 0.001:  24%|##3       | 141/600 [1:21:57<4:26:47, 34.87s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1495, Learning rate: 0.001:  24%|##3       | 142/600 [1:21:57<4:24:19, 34.63s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1639, Learning rate: 0.001:  24%|##3       | 142/600 [1:22:31<4:26:10, 34.87s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1639, Learning rate: 0.001:  24%|##3       | 143/600 [1:22:31<4:23:44, 34.63s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1515, Learning rate: 0.001:  24%|##3       | 143/600 [1:23:06<4:25:35, 34.87s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1515, Learning rate: 0.001:  24%|##4       | 144/600 [1:23:06<4:23:10, 34.63s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1686, Learning rate: 0.001:  24%|##4       | 144/600 [1:23:41<4:25:00, 34.87s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1686, Learning rate: 0.001:  24%|##4       | 145/600 [1:23:41<4:22:36, 34.63s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1534, Learning rate: 0.001:  24%|##4       | 145/600 [1:24:15<4:24:24, 34.87s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1534, Learning rate: 0.001:  24%|##4       | 146/600 [1:24:15<4:22:01, 34.63s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1461, Learning rate: 0.001:  24%|##4       | 146/600 [1:24:50<4:23:48, 34.86s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1461, Learning rate: 0.001:  24%|##4       | 147/600 [1:24:50<4:21:26, 34.63s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1559, Learning rate: 0.001:  24%|##4       | 147/600 [1:25:24<4:23:11, 34.86s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1559, Learning rate: 0.001:  25%|##4       | 148/600 [1:25:24<4:20:49, 34.62s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1606, Learning rate: 0.001:  25%|##4       | 148/600 [1:25:59<4:22:36, 34.86s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1606, Learning rate: 0.001:  25%|##4       | 149/600 [1:25:59<4:20:15, 34.62s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1409, Learning rate: 0.001:  25%|##4       | 149/600 [1:26:33<4:21:58, 34.85s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1409, Learning rate: 0.001:  25%|##5       | 150/600 [1:26:33<4:19:39, 34.62s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1494, Learning rate: 0.001:  25%|##5       | 150/600 [1:27:07<4:21:22, 34.85s/it]Loss: 0.0028, L2-loss: 0.0023, KL: 4.1494, Learning rate: 0.001:  25%|##5       | 151/600 [1:27:07<4:19:04, 34.62s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1803, Learning rate: 0.001:  25%|##5       | 151/600 [1:27:42<4:20:46, 34.85s/it]Loss: 0.0028, L2-loss: 0.0024, KL: 4.1803, Learning rate: 0.001:  25%|##5       | 152/600 [1:27:42<4:18:29, 34.62s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1331, Learning rate: 0.001:  25%|##5       | 152/600 [1:28:16<4:20:10, 34.84s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1331, Learning rate: 0.001:  26%|##5       | 153/600 [1:28:16<4:17:53, 34.62s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1215, Learning rate: 0.001:  26%|##5       | 153/600 [1:28:51<4:19:36, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1215, Learning rate: 0.001:  26%|##5       | 154/600 [1:28:51<4:17:20, 34.62s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1023, Learning rate: 0.001:  26%|##5       | 154/600 [1:29:26<4:19:02, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1023, Learning rate: 0.001:  26%|##5       | 155/600 [1:29:26<4:16:47, 34.62s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1244, Learning rate: 0.001:  26%|##5       | 155/600 [1:30:02<4:18:29, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1244, Learning rate: 0.001:  26%|##6       | 156/600 [1:30:02<4:16:15, 34.63s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1352, Learning rate: 0.001:  26%|##6       | 156/600 [1:30:37<4:17:54, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1352, Learning rate: 0.001:  26%|##6       | 157/600 [1:30:37<4:15:41, 34.63s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1000, Learning rate: 0.001:  26%|##6       | 157/600 [1:31:11<4:17:19, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1000, Learning rate: 0.001:  26%|##6       | 158/600 [1:31:11<4:15:07, 34.63s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1089, Learning rate: 0.001:  26%|##6       | 158/600 [1:31:46<4:16:44, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1089, Learning rate: 0.001:  26%|##6       | 159/600 [1:31:46<4:14:32, 34.63s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1133, Learning rate: 0.001:  26%|##6       | 159/600 [1:32:21<4:16:10, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1133, Learning rate: 0.001:  27%|##6       | 160/600 [1:32:21<4:13:59, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1337, Learning rate: 0.001:  27%|##6       | 160/600 [1:32:56<4:15:34, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1337, Learning rate: 0.001:  27%|##6       | 161/600 [1:32:56<4:13:24, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0955, Learning rate: 0.001:  27%|##6       | 161/600 [1:33:30<4:14:59, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0955, Learning rate: 0.001:  27%|##7       | 162/600 [1:33:30<4:12:50, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1070, Learning rate: 0.001:  27%|##7       | 162/600 [1:34:05<4:14:24, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1070, Learning rate: 0.001:  27%|##7       | 163/600 [1:34:05<4:12:16, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1131, Learning rate: 0.001:  27%|##7       | 163/600 [1:34:40<4:13:50, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1131, Learning rate: 0.001:  27%|##7       | 164/600 [1:34:40<4:11:43, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0996, Learning rate: 0.001:  27%|##7       | 164/600 [1:35:15<4:13:15, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0996, Learning rate: 0.001:  28%|##7       | 165/600 [1:35:15<4:11:08, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1116, Learning rate: 0.001:  28%|##7       | 165/600 [1:35:50<4:12:41, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.1116, Learning rate: 0.001:  28%|##7       | 166/600 [1:35:50<4:10:35, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0737, Learning rate: 0.001:  28%|##7       | 166/600 [1:36:25<4:12:05, 34.85s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0737, Learning rate: 0.001:  28%|##7       | 167/600 [1:36:25<4:09:59, 34.64s/it]Loss: 0.0027, L2-loss: 0.0022, KL: 4.0802, Learning rate: 0.001:  28%|##7       | 167/600 [1:36:59<4:11:29, 34.85s/it]Loss: 0.0027, L2-loss: 0.0022, KL: 4.0802, Learning rate: 0.001:  28%|##8       | 168/600 [1:36:59<4:09:25, 34.64s/it]Loss: 0.0027, L2-loss: 0.0022, KL: 4.0810, Learning rate: 0.001:  28%|##8       | 168/600 [1:37:34<4:10:53, 34.85s/it]Loss: 0.0027, L2-loss: 0.0022, KL: 4.0810, Learning rate: 0.001:  28%|##8       | 169/600 [1:37:34<4:08:49, 34.64s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0583, Learning rate: 0.001:  28%|##8       | 169/600 [1:38:08<4:10:17, 34.84s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0583, Learning rate: 0.001:  28%|##8       | 170/600 [1:38:08<4:08:14, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0920, Learning rate: 0.001:  28%|##8       | 170/600 [1:38:42<4:09:40, 34.84s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0920, Learning rate: 0.001:  28%|##8       | 171/600 [1:38:42<4:07:38, 34.64s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0867, Learning rate: 0.001:  28%|##8       | 171/600 [1:39:16<4:09:04, 34.84s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0867, Learning rate: 0.001:  29%|##8       | 172/600 [1:39:16<4:07:02, 34.63s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0425, Learning rate: 0.001:  29%|##8       | 172/600 [1:39:51<4:08:28, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0425, Learning rate: 0.001:  29%|##8       | 173/600 [1:39:51<4:06:27, 34.63s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0834, Learning rate: 0.001:  29%|##8       | 173/600 [1:40:25<4:07:52, 34.83s/it]Loss: 0.0027, L2-loss: 0.0023, KL: 4.0834, Learning rate: 0.001:  29%|##9       | 174/600 [1:40:25<4:05:52, 34.63s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0564, Learning rate: 0.001:  29%|##9       | 174/600 [1:40:59<4:07:15, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0564, Learning rate: 0.001:  29%|##9       | 175/600 [1:40:59<4:05:16, 34.63s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0496, Learning rate: 0.001:  29%|##9       | 175/600 [1:41:33<4:06:39, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0496, Learning rate: 0.001:  29%|##9       | 176/600 [1:41:33<4:04:40, 34.62s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0660, Learning rate: 0.001:  29%|##9       | 176/600 [1:42:08<4:06:03, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0660, Learning rate: 0.001:  30%|##9       | 177/600 [1:42:08<4:04:05, 34.62s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0645, Learning rate: 0.001:  30%|##9       | 177/600 [1:42:42<4:05:27, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0645, Learning rate: 0.001:  30%|##9       | 178/600 [1:42:42<4:03:30, 34.62s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0507, Learning rate: 0.001:  30%|##9       | 178/600 [1:43:17<4:04:51, 34.81s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0507, Learning rate: 0.001:  30%|##9       | 179/600 [1:43:17<4:02:55, 34.62s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0612, Learning rate: 0.001:  30%|##9       | 179/600 [1:43:52<4:04:17, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0612, Learning rate: 0.001:  30%|###       | 180/600 [1:43:52<4:02:21, 34.62s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0583, Learning rate: 0.001:  30%|###       | 180/600 [1:44:27<4:03:44, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0583, Learning rate: 0.001:  30%|###       | 181/600 [1:44:27<4:01:49, 34.63s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0459, Learning rate: 0.001:  30%|###       | 181/600 [1:45:03<4:03:11, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0459, Learning rate: 0.001:  30%|###       | 182/600 [1:45:03<4:01:16, 34.63s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0378, Learning rate: 0.001:  30%|###       | 182/600 [1:45:38<4:02:36, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0378, Learning rate: 0.001:  30%|###       | 183/600 [1:45:38<4:00:42, 34.63s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0293, Learning rate: 0.001:  30%|###       | 183/600 [1:46:13<4:02:02, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0293, Learning rate: 0.001:  31%|###       | 184/600 [1:46:13<4:00:09, 34.64s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0313, Learning rate: 0.001:  31%|###       | 184/600 [1:46:47<4:01:27, 34.82s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0313, Learning rate: 0.001:  31%|###       | 185/600 [1:46:47<3:59:34, 34.64s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0614, Learning rate: 0.001:  31%|###       | 185/600 [1:47:22<4:00:52, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0614, Learning rate: 0.001:  31%|###1      | 186/600 [1:47:22<3:59:00, 34.64s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0337, Learning rate: 0.001:  31%|###1      | 186/600 [1:47:57<4:00:18, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0337, Learning rate: 0.001:  31%|###1      | 187/600 [1:47:57<3:58:26, 34.64s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0532, Learning rate: 0.001:  31%|###1      | 187/600 [1:48:33<3:59:44, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0532, Learning rate: 0.001:  31%|###1      | 188/600 [1:48:33<3:57:53, 34.64s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0585, Learning rate: 0.001:  31%|###1      | 188/600 [1:49:08<3:59:10, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0585, Learning rate: 0.001:  32%|###1      | 189/600 [1:49:08<3:57:19, 34.65s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0403, Learning rate: 0.001:  32%|###1      | 189/600 [1:49:43<3:58:36, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0403, Learning rate: 0.001:  32%|###1      | 190/600 [1:49:43<3:56:46, 34.65s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0092, Learning rate: 0.001:  32%|###1      | 190/600 [1:50:18<3:58:02, 34.84s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0092, Learning rate: 0.001:  32%|###1      | 191/600 [1:50:18<3:56:13, 34.65s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0420, Learning rate: 0.001:  32%|###1      | 191/600 [1:50:54<3:57:28, 34.84s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0420, Learning rate: 0.001:  32%|###2      | 192/600 [1:50:54<3:55:39, 34.66s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0297, Learning rate: 0.001:  32%|###2      | 192/600 [1:51:28<3:56:53, 34.84s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0297, Learning rate: 0.001:  32%|###2      | 193/600 [1:51:28<3:55:05, 34.66s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0243, Learning rate: 0.001:  32%|###2      | 193/600 [1:52:03<3:56:17, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0243, Learning rate: 0.001:  32%|###2      | 194/600 [1:52:03<3:54:29, 34.65s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0454, Learning rate: 0.001:  32%|###2      | 194/600 [1:52:37<3:55:41, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0454, Learning rate: 0.001:  32%|###2      | 195/600 [1:52:37<3:53:54, 34.65s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0186, Learning rate: 0.001:  32%|###2      | 195/600 [1:53:11<3:55:04, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 4.0186, Learning rate: 0.001:  33%|###2      | 196/600 [1:53:11<3:53:18, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 4.0016, Learning rate: 0.001:  33%|###2      | 196/600 [1:53:46<3:54:29, 34.83s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 4.0016, Learning rate: 0.001:  33%|###2      | 197/600 [1:53:46<3:52:43, 34.65s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 3.9905, Learning rate: 0.001:  33%|###2      | 197/600 [1:54:20<3:53:54, 34.83s/it]Loss: 0.0026, L2-loss: 0.0022, KL: 3.9905, Learning rate: 0.001:  33%|###3      | 198/600 [1:54:20<3:52:08, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9857, Learning rate: 0.001:  33%|###3      | 198/600 [1:54:54<3:53:18, 34.82s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9857, Learning rate: 0.001:  33%|###3      | 199/600 [1:54:54<3:51:33, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9927, Learning rate: 0.001:  33%|###3      | 199/600 [1:55:29<3:52:43, 34.82s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9927, Learning rate: 0.001:  33%|###3      | 200/600 [1:55:29<3:50:59, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9901, Learning rate: 0.001:  33%|###3      | 200/600 [1:56:04<3:52:08, 34.82s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9901, Learning rate: 0.001:  34%|###3      | 201/600 [1:56:04<3:50:24, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9906, Learning rate: 0.001:  34%|###3      | 201/600 [1:56:39<3:51:33, 34.82s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9906, Learning rate: 0.001:  34%|###3      | 202/600 [1:56:39<3:49:50, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9951, Learning rate: 0.001:  34%|###3      | 202/600 [1:57:13<3:50:57, 34.82s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9951, Learning rate: 0.001:  34%|###3      | 203/600 [1:57:13<3:49:14, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9753, Learning rate: 0.001:  34%|###3      | 203/600 [1:57:47<3:50:21, 34.81s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9753, Learning rate: 0.001:  34%|###4      | 204/600 [1:57:47<3:48:39, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9898, Learning rate: 0.001:  34%|###4      | 204/600 [1:58:21<3:49:45, 34.81s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9898, Learning rate: 0.001:  34%|###4      | 205/600 [1:58:21<3:48:04, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9820, Learning rate: 0.001:  34%|###4      | 205/600 [1:58:56<3:49:10, 34.81s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9820, Learning rate: 0.001:  34%|###4      | 206/600 [1:58:56<3:47:29, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9600, Learning rate: 0.001:  34%|###4      | 206/600 [1:59:30<3:48:34, 34.81s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9600, Learning rate: 0.001:  34%|###4      | 207/600 [1:59:30<3:46:53, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9948, Learning rate: 0.001:  34%|###4      | 207/600 [2:00:04<3:47:58, 34.81s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9948, Learning rate: 0.001:  35%|###4      | 208/600 [2:00:04<3:46:18, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 4.0008, Learning rate: 0.001:  35%|###4      | 208/600 [2:00:39<3:47:23, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 4.0008, Learning rate: 0.001:  35%|###4      | 209/600 [2:00:39<3:45:43, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9719, Learning rate: 0.001:  35%|###4      | 209/600 [2:01:13<3:46:46, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9719, Learning rate: 0.001:  35%|###5      | 210/600 [2:01:13<3:45:07, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9996, Learning rate: 0.001:  35%|###5      | 210/600 [2:01:47<3:46:10, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9996, Learning rate: 0.001:  35%|###5      | 211/600 [2:01:47<3:44:31, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9664, Learning rate: 0.001:  35%|###5      | 211/600 [2:02:21<3:45:34, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9664, Learning rate: 0.001:  35%|###5      | 212/600 [2:02:21<3:43:56, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9822, Learning rate: 0.001:  35%|###5      | 212/600 [2:02:55<3:44:58, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9822, Learning rate: 0.001:  36%|###5      | 213/600 [2:02:55<3:43:20, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9652, Learning rate: 0.001:  36%|###5      | 213/600 [2:03:30<3:44:23, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9652, Learning rate: 0.001:  36%|###5      | 214/600 [2:03:30<3:42:46, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9418, Learning rate: 0.001:  36%|###5      | 214/600 [2:04:05<3:43:49, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9418, Learning rate: 0.001:  36%|###5      | 215/600 [2:04:05<3:42:12, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9702, Learning rate: 0.001:  36%|###5      | 215/600 [2:04:39<3:43:14, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9702, Learning rate: 0.001:  36%|###6      | 216/600 [2:04:39<3:41:37, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9659, Learning rate: 0.001:  36%|###6      | 216/600 [2:05:14<3:42:39, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9659, Learning rate: 0.001:  36%|###6      | 217/600 [2:05:14<3:41:02, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9829, Learning rate: 0.001:  36%|###6      | 217/600 [2:05:48<3:42:03, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9829, Learning rate: 0.001:  36%|###6      | 218/600 [2:05:48<3:40:27, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9677, Learning rate: 0.001:  36%|###6      | 218/600 [2:06:23<3:41:29, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9677, Learning rate: 0.001:  36%|###6      | 219/600 [2:06:23<3:39:53, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9664, Learning rate: 0.001:  36%|###6      | 219/600 [2:06:58<3:40:54, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9664, Learning rate: 0.001:  37%|###6      | 220/600 [2:06:58<3:39:19, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9410, Learning rate: 0.001:  37%|###6      | 220/600 [2:07:33<3:40:19, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9410, Learning rate: 0.001:  37%|###6      | 221/600 [2:07:33<3:38:45, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9540, Learning rate: 0.001:  37%|###6      | 221/600 [2:08:08<3:39:45, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9540, Learning rate: 0.001:  37%|###7      | 222/600 [2:08:08<3:38:11, 34.63s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9782, Learning rate: 0.001:  37%|###7      | 222/600 [2:08:43<3:39:11, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9782, Learning rate: 0.001:  37%|###7      | 223/600 [2:08:43<3:37:38, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9558, Learning rate: 0.001:  37%|###7      | 223/600 [2:09:19<3:38:37, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9558, Learning rate: 0.001:  37%|###7      | 224/600 [2:09:19<3:37:04, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9416, Learning rate: 0.001:  37%|###7      | 224/600 [2:09:54<3:38:03, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9416, Learning rate: 0.001:  38%|###7      | 225/600 [2:09:54<3:36:31, 34.64s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9438, Learning rate: 0.001:  38%|###7      | 225/600 [2:10:29<3:37:29, 34.80s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9438, Learning rate: 0.001:  38%|###7      | 226/600 [2:10:29<3:35:57, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9457, Learning rate: 0.001:  38%|###7      | 226/600 [2:11:04<3:36:55, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9457, Learning rate: 0.001:  38%|###7      | 227/600 [2:11:04<3:35:23, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9441, Learning rate: 0.001:  38%|###7      | 227/600 [2:11:39<3:36:20, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9441, Learning rate: 0.001:  38%|###8      | 228/600 [2:11:39<3:34:49, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9439, Learning rate: 0.001:  38%|###8      | 228/600 [2:12:14<3:35:46, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9439, Learning rate: 0.001:  38%|###8      | 229/600 [2:12:14<3:34:15, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9612, Learning rate: 0.001:  38%|###8      | 229/600 [2:12:50<3:35:12, 34.80s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9612, Learning rate: 0.001:  38%|###8      | 230/600 [2:12:50<3:33:41, 34.65s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9138, Learning rate: 0.001:  38%|###8      | 230/600 [2:13:24<3:34:37, 34.80s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9138, Learning rate: 0.001:  38%|###8      | 231/600 [2:13:24<3:33:06, 34.65s/it]Loss: 0.0024, L2-loss: 0.0021, KL: 3.9361, Learning rate: 0.001:  38%|###8      | 231/600 [2:13:58<3:34:01, 34.80s/it]Loss: 0.0024, L2-loss: 0.0021, KL: 3.9361, Learning rate: 0.001:  39%|###8      | 232/600 [2:13:58<3:32:31, 34.65s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9227, Learning rate: 0.001:  39%|###8      | 232/600 [2:14:33<3:33:26, 34.80s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9227, Learning rate: 0.001:  39%|###8      | 233/600 [2:14:33<3:31:56, 34.65s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9321, Learning rate: 0.001:  39%|###8      | 233/600 [2:15:07<3:32:50, 34.80s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9321, Learning rate: 0.001:  39%|###9      | 234/600 [2:15:07<3:31:21, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9420, Learning rate: 0.001:  39%|###9      | 234/600 [2:15:41<3:32:14, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9420, Learning rate: 0.001:  39%|###9      | 235/600 [2:15:41<3:30:45, 34.65s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9488, Learning rate: 0.001:  39%|###9      | 235/600 [2:16:15<3:31:38, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9488, Learning rate: 0.001:  39%|###9      | 236/600 [2:16:15<3:30:10, 34.64s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9327, Learning rate: 0.001:  39%|###9      | 236/600 [2:16:50<3:31:03, 34.79s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9327, Learning rate: 0.001:  40%|###9      | 237/600 [2:16:50<3:29:35, 34.64s/it]Loss: 0.0024, L2-loss: 0.0021, KL: 3.9387, Learning rate: 0.001:  40%|###9      | 237/600 [2:17:24<3:30:28, 34.79s/it]Loss: 0.0024, L2-loss: 0.0021, KL: 3.9387, Learning rate: 0.001:  40%|###9      | 238/600 [2:17:24<3:29:00, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9360, Learning rate: 0.001:  40%|###9      | 238/600 [2:17:59<3:29:53, 34.79s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9360, Learning rate: 0.001:  40%|###9      | 239/600 [2:17:59<3:28:25, 34.64s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9419, Learning rate: 0.001:  40%|###9      | 239/600 [2:18:33<3:29:17, 34.78s/it]Loss: 0.0025, L2-loss: 0.0021, KL: 3.9419, Learning rate: 0.001:  40%|####      | 240/600 [2:18:33<3:27:50, 34.64s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9244, Learning rate: 0.001:  40%|####      | 240/600 [2:19:07<3:28:41, 34.78s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9244, Learning rate: 0.001:  40%|####      | 241/600 [2:19:07<3:27:15, 34.64s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9128, Learning rate: 0.001:  40%|####      | 241/600 [2:19:42<3:28:06, 34.78s/it]Loss: 0.0024, L2-loss: 0.0020, KL: 3.9128, Learning rate: 0.001:  40%|####      | 242/600 [2:19:42<3:26:39, 34.64s/it]2018-07-30 02:28:47.951555: E tensorflow/core/kernels/check_numerics_op.cc:185] abnormal_detected_host @0x2b8e3b205600 = {1, 0} vae/shared_encoder/conv-02/weights/read:0
Traceback (most recent call last):
  File "main.py", line 21, in <module>
    x, x_org, nan = vae.train()
  File "/cluster/home/leoj/sandbox/vae.py", line 257, in train
    loss, recon_loss, kl_loss, loss_labels = self.update(x, kl_loss_mult, learning_rate, keep_prob)
  File "/cluster/home/leoj/sandbox/vae.py", line 222, in update
    self._learning_rate: learning_rate, self.keep_prob: keep_prob})
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: vae/shared_encoder/conv-02/weights/read:0 : Tensor had NaN values
	 [[Node: vae/CheckNumerics_46 = CheckNumerics[T=DT_FLOAT, message="vae/shared_encoder/conv-02/weights/read:0", _device="/job:localhost/replica:0/task:0/device:GPU:0"](vae/shared_encoder/conv-02/weights/read, ^vae/CheckNumerics_45)]]

Caused by op u'vae/CheckNumerics_46', defined at:
  File "main.py", line 9, in <module>
    vae = VAE(latent_dim=config.latent_dim, batch_size=config.batch_size, config=config)
  File "/cluster/home/leoj/sandbox/vae.py", line 23, in __init__
    self._build_graph()
  File "/cluster/home/leoj/sandbox/vae.py", line 138, in _build_graph
    self.check = tf.add_check_numerics_ops()
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/ops/numerics.py", line 99, in add_check_numerics_ops
    check_op = [array_ops.check_numerics(output, message=message)]
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 734, in check_numerics
    "CheckNumerics", tensor=tensor, message=message, name=name)
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_op
    op_def=op_def)
  File "/cluster/home/leoj/.virtualenvs/mp/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): vae/shared_encoder/conv-02/weights/read:0 : Tensor had NaN values
	 [[Node: vae/CheckNumerics_46 = CheckNumerics[T=DT_FLOAT, message="vae/shared_encoder/conv-02/weights/read:0", _device="/job:localhost/replica:0/task:0/device:GPU:0"](vae/shared_encoder/conv-02/weights/read, ^vae/CheckNumerics_45)]]

